from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator
from airflow.operators.python import PythonOperator
from airflow.kubernetes.secret import Secret
from kubernetes.client import models as k8s
from kubernetes import client, config


EXPERIMENT_NAME = "{{ dag_name }}"

args = {
    'owner': 'airflow',
}

with DAG(
    dag_id='{{ dag_name }}',
    default_args=args,
    schedule_interval=None,
    start_date=days_ago(2),
    tags=['demo', 'commit_sha:{{ git_info.commit_sha }}'],
    params={"example_key": "example_value"},
) as dag:

    pvc_name = '{{ project_name | safe | slugify }}.{{ execution_marker }}'

    def start_mlflow_run(ti, **kwargs):
        from mlflow.tracking import MlflowClient

        client = MlflowClient("{{ mlflow_url }}")
        experiment_id = client.get_experiment_by_name(EXPERIMENT_NAME).experiment_id
        run_id = client.create_run(experiment_id).info.run_id
        ti.xcom_push('mlflow_run_id', run_id)

    start_mlflow_run = PythonOperator(
        task_id='start_mlflow_run',
        python_callable=start_mlflow_run,
    )

    def create_pipeline_storage(ti, **kwargs):
        with client.ApiClient(config.load_incluster_config()) as api_client:
            v1 = client.CoreV1Api(api_client)

            pvc = k8s.V1PersistentVolumeClaim(metadata=k8s.V1ObjectMeta(name=pvc_name, namespace='{{ config.namespace }}'),
                    spec=k8s.V1PersistentVolumeClaimSpec(access_modes=['ReadWriteOnce'],
                    resources=k8s.V1ResourceRequirements(requests={'storage':'{{ config.request_storage | safe }}'})))

            v1.create_namespaced_persistent_volume_claim('{{ config.namespace }}', pvc)

    create_pipeline_storage = PythonOperator(
        task_id='create_pipeline_storage',
        python_callable=create_pipeline_storage
    )

    def delete_pipeline_storage(ti, **kwargs):
        with client.ApiClient(config.load_incluster_config()) as api_client:
            v1 = client.CoreV1Api(api_client)
            v1.delete_namespaced_persistent_volume_claim(name=pvc_name, namespace='{{ config.namespace }}')

    delete_pipeline_storage = PythonOperator(
        task_id='delete_pipeline_storage',
        python_callable=delete_pipeline_storage,
        trigger_rule='all_done'
    )

    volume = k8s.V1Volume(name='storage',persistent_volume_claim=k8s.V1PersistentVolumeClaimVolumeSource(claim_name=pvc_name))
    volume_mount = k8s.V1VolumeMount(name='storage', mount_path='/data', sub_path=None, read_only=False)
    tasks = {}
    {% for node in pipeline.nodes %}
    tasks["{{ node.name | safe | slugify }}"] = KubernetesPodOperator(
        task_id="{{ node.name | safe | slugify  }}",
        name="{{ node.name | safe | slugify  }}",
        namespace='{{ config.namespace }}',
        image='{{ config.image }}',
        image_pull_policy='{{ config.image_pull_policy }}',
        cmds=['bash', '-c'],
        arguments=['MLFLOW_RUN_ID={% raw %}{{ task_instance.xcom_pull(key="mlflow_run_id") }}{% endraw %} kedro run -e {{ env }} --node \'{{ node.name | safe }}\''],
        is_delete_operator_pod=True,
        volumes=[volume],
        volume_mounts=[volume_mount],
    )
    {% endfor %}

    
    {% for parent_node, child_nodes in dependencies.items() -%}
    {% for child in child_nodes %}
    tasks["{{ parent_node.name | safe | slugify }}"] >> tasks["{{ child.name | safe | slugify }}"]
    {% endfor %}
    {%- endfor %}

    {% for node in base_nodes %}
    start_mlflow_run >> tasks['{{ node | slugify }}']
    create_pipeline_storage >> tasks['{{ node | slugify }}']
    {% endfor %}

    {% for node in bottom_nodes %}
    tasks['{{ node | slugify }}'] >> delete_pipeline_storage
    {% endfor %}


if __name__ == "__main__":
    dag.cli()
